# ProjetoBD# ðŸ’‰ Pipeline de Dados: AnÃ¡lise ExploratÃ³ria de Dados dos Servidores da Prefeitura da Cidade do Recife

> **Projeto de Banco de Dados (2025.2) - CIn/UFPE**



Este projeto implementa e compara duas arquiteturas fundamentais de Engenharia de Dados â€” **ETL ClÃ¡ssico** (Python/Pandas) e **ELT Moderno** (dbt/SQL) â€” para processar, higienizar e modelar dados pÃºblicos de vacinaÃ§Ã£o.

O diferencial deste projeto Ã© a implementaÃ§Ã£o final de uma **Modelagem Dimensional (Esquema Estrela)** , transformando milhÃµes de registros brutos em um Data Warehouse otimizado para Business Intelligence (BI).

-----

## ðŸŽ¯ Objetivo e Desafio

O objetivo foi integrar dados dispersos temporalmente para permitir anÃ¡lises histÃ³ricas.

  * **Fonte:** [Portal de Dados Abertos do Recife](http://dados.recife.pe.gov.br/dataset/servidores)
  * **Dados Brutos:** Arquivos CSV separados por ano (2022, 2023, 2024) contendo registros de vacinaÃ§Ã£o.
  * **Desafio Principal:** Os dados possuiam inconsistÃªncias de formato, colunas "sujas" (misturando mÃºltiplos dados em uma string) e ausÃªncia de chaves primÃ¡rias confiÃ¡veis.

## ðŸ—ï¸ Arquitetura da SoluÃ§Ã£o

O projeto constrÃ³i o mesmo modelo final atravÃ©s de dois caminhos distintos para fins de comparaÃ§Ã£o:

### 1\. Abordagem ETL (Python Driven)

  * **ExtraÃ§Ã£o:** Leitura automatizada dos CSVs.
  * **TransformaÃ§Ã£o:** Limpeza, deduplicaÃ§Ã£o e modelagem dimensional realizadas inteiramente em memÃ³ria usando **Pandas**.
  * **Carga:** InserÃ§Ã£o das tabelas finais no PostgreSQL usando SQLAlchemy.

### 2\. Abordagem ELT (Modern Data Stack)

  * **ExtraÃ§Ã£o & Carga (EL):** Python Ã© usado apenas para carregar os dados brutos (`raw`) no banco.
  * **TransformaÃ§Ã£o (T):** O **dbt (data build tool)** orquestra transformaÃ§Ãµes complexas diretamente no banco de dados usando SQL:
      * **Staging:** UnificaÃ§Ã£o dos anos (`UNION ALL`).
      * **Intermediate:** Limpeza pesada (Regex, Split, Case When).
      * **Marts:** CriaÃ§Ã£o das Tabelas Fato e DimensÃ£o.

-----

## â­ Modelagem de Dados (Esquema Estrela)

Ao final do pipeline, os dados sÃ£o organizados em um modelo dimensional para facilitar anÃ¡lises:


-----

## ðŸ› ï¸ Tecnologias Utilizadas

  *  **Python 3.10+**: Scripting e manipulaÃ§Ã£o de dados (Pandas).
  *  **PostgreSQL**: Data Warehouse.
  *  **dbt Core**: OrquestraÃ§Ã£o de transformaÃ§Ãµes SQL e testes de dados.
  * **SQLAlchemy & Psycopg2**: Conectores de banco de dados.
  * **Git/GitHub**: Versionamento de cÃ³digo.

-----

## ðŸ“‚ Estrutura do RepositÃ³rio

```
.
â”œâ”€â”€ analysis/                     # Scripts SQL com as anÃ¡lises finais (Insights)
â”œâ”€â”€ data/                         # Arquivos CSV brutos (ignorados no git)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ ETL.ipynb                 # Pipeline 1: ETL completo em Python
â”‚   â””â”€â”€ ELT_load.ipynb            # Pipeline 2: Carga bruta para o dbt
â””â”€â”€ README.md
```

-----

## ðŸš€ Como Executar

### PrÃ©-requisitos

1.  Instale Python e PostgreSQL.
2.  Clone este repositÃ³rio.
3.  Instale as dependÃªncias: `pip install pandas sqlalchemy psycopg2-binary dbt-postgres`.

### Passo 1: Carga Inicial (EL)

Execute o notebook `notebooks/ELT_load.ipynb`. Isso lerÃ¡ os CSVs da pasta `data/` e criarÃ¡ as tabelas `raw_Servidores` no seu banco de dados.

### Passo 2: ConfiguraÃ§Ã£o do dbt

1.  Configure seu arquivo `profiles.yml` (geralmente em `~/.dbt/`) com as credenciais do seu PostgreSQL local.
2.  No terminal, navegue atÃ© a pasta do projeto dbt:
    ```bash
    cd transformacao_vacinados
    ```
3.  Teste a conexÃ£o:
    ```bash
    dbt debug
    ```

### Passo 3: ExecuÃ§Ã£o das TransformaÃ§Ãµes

Ainda no terminal, execute o comando para construir o Data Warehouse:

```bash
dbt run
```

*Isso criarÃ¡ todas as views de staging e as tabelas Fato e DimensÃ£o finais.*

-----

## ðŸ“Š Resultados e Insights

As consultas SQL na pasta `/analysis` demonstram o poder do modelo construÃ­do:



>**Alunos:**
> Denilson
> Janderson
> Jean
> Leonardo
> Lucas Matheus
> Luiz Miguel