# üíâ Pipeline de Dados: An√°lise Explorat√≥ria de Dados dos Servidores da Prefeitura da Cidade do Recife

> **Projeto de Banco de Dados (2025.2) - CIn/UFPE**

Este projeto implementa e compara duas arquiteturas fundamentais de Engenharia de Dados ‚Äî **ETL Cl√°ssico** (Python/Pandas) e **ELT Moderno** (dbt/SQL) ‚Äî para processar, higienizar e modelar dados p√∫blicos de servidores municipais.

O diferencial deste projeto √© a implementa√ß√£o final de uma **Modelagem Dimensional (Esquema Estrela)**, transformando milh√µes de registros brutos em um Data Warehouse otimizado para Business Intelligence (BI).

-----

## üéØ Objetivo e Desafio

O objetivo foi integrar dados dispersos temporalmente para permitir an√°lises hist√≥ricas.

* **Fonte:** [Portal de Dados Abertos do Recife](http://dados.recife.pe.gov.br/dataset/servidores)
* **Dados Brutos:** Arquivos CSV separados por ano (2019, 2020, 2021) contendo registros de folha de pagamento.
* **Desafio Principal:** Os dados possuiam inconsist√™ncias de formato, colunas "sujas" e aus√™ncia de chaves prim√°rias confi√°veis.

## üèóÔ∏è Arquitetura da Solu√ß√£o

O projeto constr√≥i o mesmo modelo final atrav√©s de dois caminhos distintos para fins de compara√ß√£o:

### 1. Abordagem ETL (Python Driven)

* **Extra√ß√£o:** Leitura automatizada dos CSVs brutos.
* **Transforma√ß√£o:** Limpeza, deduplica√ß√£o e **modelagem dimensional (Star Schema)** realizadas inteiramente em mem√≥ria usando **Pandas**.
    * Cria√ß√£o de IDs substitutos (Surrogate Keys).
    * Normaliza√ß√£o de tabelas (Cargo, Lota√ß√£o, Servidor).
* **Carga:** Inser√ß√£o das tabelas Fato e Dimens√£o finais no PostgreSQL usando SQLAlchemy.

### 2. Abordagem ELT (Modern Data Stack)

* **Extra√ß√£o & Carga (EL):** Python √© usado apenas para carregar os dados brutos (`raw`) no banco.
* **Transforma√ß√£o (T):** O **dbt (data build tool)** orquestra transforma√ß√µes complexas diretamente no banco de dados usando SQL:
    * **Staging:** Unifica√ß√£o dos anos (`UNION ALL`).
    * **Intermediate:** Limpeza pesada (Regex, Split, Case When).
    * **Marts:** Cria√ß√£o das Tabelas Fato e Dimens√£o.

-----

## ‚≠ê Modelagem de Dados (Esquema Estrela)

Ao final do pipeline, os dados s√£o organizados no seguinte modelo dimensional:

* **Fato:** `fato_folha` (Granularidade: Servidor/M√™s)
* **Dimens√µes:** `dim_servidor`, `dim_lotacao`, `dim_cargo`, `dim_situacao`, `dim_tempo`.

-----

## üõ†Ô∏è Tecnologias Utilizadas

* **Python 3.10+**: Scripting e manipula√ß√£o de dados (Pandas).
* **PostgreSQL**: Data Warehouse (via Docker).
* **dbt Core**: Orquestra√ß√£o de transforma√ß√µes SQL.
* **SQLAlchemy**: Conectores de banco de dados.
* **Docker & Docker Compose**: Containeriza√ß√£o do ambiente.

-----

## üöÄ Como Executar

### Pr√©-requisitos

1.  Instale Docker e Docker Compose (ou tenha um Postgres local).
2.  Clone este reposit√≥rio.
3.  Crie um arquivo `.env` na raiz do projeto (copie de `.env.example`):
    ```bash
    cp .env.example .env
    ```
4.  Instale as depend√™ncias Python:
    ```bash
    pip install pandas sqlalchemy psycopg2-binary dbt-postgres python-dotenv
    ```

### Passo 1: Subir o Banco de Dados

Utilize o Docker para iniciar o PostgreSQL configurado:
```bash
docker-compose up -d
