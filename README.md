# ğŸ’‰ Pipeline de Dados: AnÃ¡lise ExploratÃ³ria de Dados dos Servidores da Prefeitura da Cidade do Recife

**Projeto de Banco de Dados (2025.2) - CIn/UFPE**

Este projeto implementa e compara duas arquiteturas fundamentais de Engenharia de Dados â€” **ETL ClÃ¡ssico** (Python/Pandas) e **ELT Moderno** (dbt/SQL) â€” para processar, higienizar e modelar dados pÃºblicos de servidores municipais.

O diferencial deste projeto Ã© a implementaÃ§Ã£o final de uma **Modelagem Dimensional (Esquema Estrela)**, transformando milhÃµes de registros brutos em um Data Warehouse otimizado para Business Intelligence (BI), alÃ©m de scripts de auditoria de qualidade de dados.

-----

## ğŸ¯ Objetivo e Desafio

O objetivo foi integrar dados dispersos temporalmente para permitir anÃ¡lises histÃ³ricas.

* **Fonte:** [Portal de Dados Abertos do Recife](http://dados.recife.pe.gov.br/dataset/servidores)
* **Dados Brutos:** Arquivos CSV separados por ano (2019, 2020, 2021) contendo registros de folha de pagamento.
* **Desafio Principal:** Os dados possuiam inconsistÃªncias de formato, colunas "sujas" (ex: valores nulos acima de 90%) e ausÃªncia de chaves primÃ¡rias confiÃ¡veis.

-----

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

O projeto constrÃ³i o mesmo modelo final atravÃ©s de dois caminhos distintos para fins de comparaÃ§Ã£o:

### 1. Abordagem ETL (Python Driven)

* **ExtraÃ§Ã£o:** Leitura automatizada dos CSVs brutos com tratamento de formataÃ§Ã£o (leitura bruta para correÃ§Ã£o de aspas).
* **TransformaÃ§Ã£o (Pandas):**
    * Limpeza avanÃ§ada e tipagem de dados.
    * DeduplicaÃ§Ã£o e tratamento de dimensÃµes (SCD Tipo 1).
    * **Modelagem Dimensional:** CriaÃ§Ã£o de Tabelas Fato e DimensÃ£o em memÃ³ria.
* **Carga:** InserÃ§Ã£o otimizada no PostgreSQL via SQLAlchemy.

### 2. Abordagem ELT (Modern Data Stack)

* **ExtraÃ§Ã£o & Carga (EL):** Python Ã© usado apenas para carregar os dados brutos (`raw`) no banco.
* **TransformaÃ§Ã£o (T):** O **dbt (data build tool)** orquestra transformaÃ§Ãµes complexas diretamente no banco de dados:
    * **Staging:** UnificaÃ§Ã£o dos anos e padronizaÃ§Ã£o de tipos.
    * **Intermediate:** Regras de negÃ³cio e limpeza via SQL.
    * **Marts:** MaterializaÃ§Ã£o do Esquema Estrela.

-----

## ğŸ“‚ Estrutura do Projeto

```
â”œâ”€â”€ analysis/           # Consultas SQL para insights (Outliers, Sazonalidade)
â”œâ”€â”€ checks/             # Scripts SQL de auditoria e validaÃ§Ã£o cruzada
â”œâ”€â”€ data/               # Arquivos CSV (Dados Brutos)
â”œâ”€â”€ models/             # Modelos dbt (Staging e Marts)
â”œâ”€â”€ notebooks/          # Jupyter Notebooks (ETL.ipynb e ELT.ipynb)
â”œâ”€â”€ docker-compose.yml  # ConfiguraÃ§Ã£o do Banco de Dados (Opcional)
â”œâ”€â”€ requirements.txt    # DependÃªncias do Python
â””â”€â”€ dbt_project.yml     # ConfiguraÃ§Ã£o do dbt
````

-----

## ğŸš€ Como Executar

### 1\. PreparaÃ§Ã£o do Ambiente Python

Clone o repositÃ³rio e instale as dependÃªncias listadas.

```
# Clone o repositÃ³rio
# Crie um ambiente virtual (recomendado)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Instale as dependÃªncias
pip install -r requirements.txt
```

### 2\. ConfiguraÃ§Ã£o do Banco de Dados

VocÃª precisa de um banco de dados PostgreSQL rodando. Escolha uma das opÃ§Ãµes abaixo:

#### OpÃ§Ã£o A: Usando Docker

Se vocÃª tem Docker instalado, basta rodar o comando abaixo para subir um banco configurado automaticamente:

```
docker-compose up -d
```

#### OpÃ§Ã£o B: Usando um Banco Local existente

Se vocÃª jÃ¡ tem o PostgreSQL instalado na sua mÃ¡quina:

1.  Crie um banco de dados vazio (ex: `servidores_recife`).
2.  Garanta que as credenciais no arquivo `.env` apontem para o seu banco local.

### 3\. ConfiguraÃ§Ã£o de VariÃ¡veis de Ambiente

Crie um arquivo `.env` na raiz do projeto baseando-se no exemplo:

```
cp .env.example .env
```

Edite o arquivo `.env` e ajuste as credenciais (`POSTGRES_USER`, `POSTGRES_PASSWORD`, etc.) conforme a opÃ§Ã£o de banco escolhida acima.

-----

## ğŸ““ Executando os Notebooks

Os processos de ingestÃ£o e tratamento estÃ£o documentados em **Jupyter Notebooks** na pasta `notebooks/`.

### ExecuÃ§Ã£o Local (VS Code ou Jupyter Lab)

Basta abrir os arquivos `ETL.ipynb` ou `ELT.ipynb` e executar as cÃ©lulas sequencialmente. O Kernel do Python deve estar utilizando o ambiente virtual onde as dependÃªncias foram instaladas.

### âš ï¸ ExecuÃ§Ã£o no Google Colab

Se vocÃª optar por rodar no Google Colab:

1.  **DependÃªncias:** O arquivo `requirements.txt` nÃ£o Ã© lido automaticamente. VocÃª deve executar o seguinte comando na primeira cÃ©lula:
    ```
    !pip install pandas sqlalchemy psycopg2-binary dbt-postgres python-dotenv
    ```
2.  **Arquivos:** VocÃª precisarÃ¡ fazer o upload manual da pasta `data/` (com os CSVs) e do arquivo `.env` para o ambiente de execuÃ§Ã£o do Colab.
3.  **ConexÃ£o:** Certifique-se de que o Colab consiga acessar seu banco de dados (se o banco for local, vocÃª precisarÃ¡ usar um tÃºnel como o *ngrok* ou migrar o banco para a nuvem, como AWS RDS ou Supabase).

-----

## ğŸƒ Executando o Pipeline Completo (dbt)

ApÃ³s rodar a carga inicial via notebooks, vocÃª pode gerenciar as transformaÃ§Ãµes ELT via CLI do dbt:

```
# Executar todas as transformaÃ§Ãµes (CriaÃ§Ã£o de Tabelas/Views)
dbt run
```

-----

## ğŸ” AnÃ¡lises DisponÃ­veis

ApÃ³s a execuÃ§Ã£o, vocÃª pode rodar as consultas SQL disponÃ­veis na pasta `analysis/` diretamente no seu cliente de banco de dados (DBeaver, pgAdmin) para gerar insights:

  * **`salario_acima_media.sql`**: Detecta salÃ¡rios discrepantes (Z-Score \> 3).
  * **`variacao_salario.sql`**: Analisa aumentos bruscos (\>100%) de um mÃªs para o outro.
  * **`idosos_ativos.sql`**: Identifica servidores com tempo de casa excessivo sem aposentadoria.

-----

## ğŸ› ï¸ Tecnologias

  * **Linguagem:** Python 3.10+
  * **Banco de Dados:** PostgreSQL 15
  * **Engenharia de Dados:** Pandas, SQLAlchemy, dbt
  * **Infraestrutura:** Docker (Opcional)