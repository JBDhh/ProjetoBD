# üíâ Pipeline de Dados: An√°lise Explorat√≥ria de Dados dos Servidores da Prefeitura da Cidade do Recife

> **Projeto de Banco de Dados (2025.2) - CIn/UFPE**

Este projeto implementa e compara duas arquiteturas fundamentais de Engenharia de Dados ‚Äî **ETL Cl√°ssico** (Python/Pandas) e **ELT Moderno** (dbt/SQL) ‚Äî para processar, higienizar e modelar dados p√∫blicos de servidores municipais.

O diferencial deste projeto √© a implementa√ß√£o final de uma **Modelagem Dimensional (Esquema Estrela)**, transformando milh√µes de registros brutos em um Data Warehouse otimizado para Business Intelligence (BI), al√©m de scripts de auditoria de qualidade de dados.

-----

## üéØ Objetivo e Desafio

O objetivo foi integrar dados dispersos temporalmente para permitir an√°lises hist√≥ricas.

* **Fonte:** [Portal de Dados Abertos do Recife](http://dados.recife.pe.gov.br/dataset/servidores)
* **Dados Brutos:** Arquivos CSV separados por ano (2019, 2020, 2021) contendo registros de folha de pagamento.
* **Desafio Principal:** Os dados possuiam inconsist√™ncias de formato, colunas "sujas" (ex: valores nulos acima de 90%) e aus√™ncia de chaves prim√°rias confi√°veis.

-----

## üèóÔ∏è Arquitetura da Solu√ß√£o

O projeto constr√≥i o mesmo modelo final atrav√©s de dois caminhos distintos para fins de compara√ß√£o:

### 1. Abordagem ETL (Python Driven)

* **Extra√ß√£o:** Leitura automatizada dos CSVs brutos.
* **Transforma√ß√£o (Pandas):**
    * Limpeza avan√ßada: Remo√ß√£o de colunas esparsas (>90% zeros/nulos) e tratamento de erros de formata√ß√£o (aspas residuais).
    * Deduplica√ß√£o e tratamento de dimens√µes (SCD Tipo 1).
    * **Modelagem Dimensional:** Cria√ß√£o de Tabelas Fato e Dimens√£o em mem√≥ria.
* **Carga:** Inser√ß√£o otimizada no PostgreSQL via SQLAlchemy.

### 2. Abordagem ELT (Modern Data Stack)

* **Extra√ß√£o & Carga (EL):** Python √© usado apenas para carregar os dados brutos (`raw`) no banco.
* **Transforma√ß√£o (T):** O **dbt (data build tool)** orquestra transforma√ß√µes complexas diretamente no banco de dados:
    * **Staging:** Unifica√ß√£o dos anos e padroniza√ß√£o de tipos.
    * **Intermediate:** Regras de neg√≥cio e limpeza via SQL.
    * **Marts:** Materializa√ß√£o do Esquema Estrela.

-----

## ‚≠ê Modelagem de Dados (Esquema Estrela)

Ao final do pipeline, os dados s√£o organizados no seguinte modelo dimensional:

* **Fato:** `fato_folha` (Granularidade: Servidor/M√™s)
* **Dimens√µes:** `dim_servidor`, `dim_lotacao`, `dim_cargo`, `dim_situacao`, `dim_tempo`.

-----

## üîé An√°lises e Qualidade de Dados

O projeto inclui m√≥dulos de an√°lise explorat√≥ria para auditoria da base:

* **Detec√ß√£o de Outliers Salariais:** Script SQL dedicado (`analysis/salario_acima_media.sql`) que utiliza c√°lculo de **Z-score** (desvio-padr√£o) para identificar pagamentos an√¥malos ou erros de digita√ß√£o no sistema original (ex: sal√°rios acima de 3 desvios da m√©dia do cargo).

-----

## üõ†Ô∏è Tecnologias Utilizadas

* **Python 3.10+**: Scripting e manipula√ß√£o de dados (Pandas).
* **PostgreSQL**: Data Warehouse (via Docker).
* **dbt Core**: Orquestra√ß√£o de transforma√ß√µes SQL e testes de dados.
* **SQLAlchemy**: Conectores de banco de dados.
* **Docker & Docker Compose**: Containeriza√ß√£o do ambiente.

-----

## üöÄ Como Executar

### Pr√©-requisitos

1.  Instale Docker e Docker Compose.
2.  Clone este reposit√≥rio.
3.  Crie um arquivo `.env` na raiz do projeto (copie de `.env.example`):
    ```bash
    cp .env.example .env
    ```
4.  Instale as depend√™ncias Python:
    ```bash
    pip install pandas sqlalchemy psycopg2-binary dbt-postgres python-dotenv
    ```

### Passo 1: Subir o Banco de Dados

Utilize o Docker para iniciar o PostgreSQL configurado:
```bash
docker-compose up -d